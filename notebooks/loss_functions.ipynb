{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions \n",
    "\n",
    "Loss functions are used to train neural networks. They measure the difference between the output of the network and the desired output. The loss function is a key part of the training process because it is the guide to the network about how to update the weights. The loss function takes in the (output, target) pair of inputs and computes a value that estimates how far away the output is from the target. The higher the loss value, the more different the output is from the target. The goal of training is to reduce this loss value.\n",
    "\n",
    "Different loss functions:\n",
    "\n",
    "   1. Mean Squared Error (MSE)\n",
    "   2. Mean Absolute Error (MAE)\n",
    "   3. Huber Loss\n",
    "   4. Cross Entropy Loss\n",
    "   5. Binary Cross Entropy Loss\n",
    "   6. Kullback-Leibler Divergence Loss\n",
    "   7. Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "MSE is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values. The MSE is a measure of the quality of an estimator. It is always non-negative, and values closer to zero are better.\n",
    "\n",
    "Equation:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "where $y_i$ is the target value and $\\hat{y_i}$ is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6584520478375584\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "y_true = np.random.normal(0, 1, 100)\n",
    "y_pred = np.random.normal(0, 1, 100)\n",
    "print(mse_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mean Absolute Error (MAE)\n",
    "\n",
    "MAE is the average of the absolute difference between the target value and the value predicted by the model. It is the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
    "\n",
    "Equation:\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y_i}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.024812570272987\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "def mae_loss(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "y_true = np.random.normal(0, 1, 100)\n",
    "y_pred = np.random.normal(0, 1, 100)\n",
    "print(mae_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Huber Loss\n",
    "\n",
    "Huber loss is less sensitive to outliers in data than the squared error loss. It is a loss function used in robust regression, that is less sensitive to outliers in data than the squared error loss. The Huber loss combines MSE and MAE. It is quadratic for small values of the error and linear for large values.\n",
    "\n",
    "Equation:\n",
    "\n",
    "$$L_{\\delta}(y, f(x)) = \\begin{cases} \\frac{1}{2}(y - f(x))^2 & \\text{for } |y - f(x)| \\leq \\delta \\\\ \\delta|y - f(x)| - \\frac{1}{2}\\delta^2 & \\text{otherwise} \\end{cases}$$\n",
    "\n",
    "where $y$ is the target value, $f(x)$ is the predicted value and $\\delta$ is the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8757908259200415\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    return np.mean(np.where(np.abs(y_true - y_pred) < delta, 0.5 * np.power(y_true - y_pred, 2), delta * (np.abs(y_true - y_pred) - 0.5 * delta)))\n",
    "\n",
    "y_true = np.random.normal(0, 1, 100)\n",
    "y_pred = np.random.normal(0, 1, 100)\n",
    "print(huber_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Cross Entropy Loss\n",
    "\n",
    "Cross entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0. Cross entropy loss is used when we have two or more label classes.\n",
    "\n",
    "Equation:\n",
    "\n",
    "$L_{CE} = -\\sum_{i=1}^{n}y_i\\log(\\hat{y_i})$\n",
    "\n",
    "where $y_i$ is the target value and $\\hat{y_i}$ is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross entropy loss: 0.0446\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    return -np.mean(y_true * np.log(y_pred))\n",
    "\n",
    "y_true = [0, 0, 1, 0, 0]\n",
    "y_pred = [0.05, 0.05, 0.8, 0.05, 0.05]\n",
    "print(f'Cross entropy loss: {cross_entropy_loss(y_true, y_pred):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
