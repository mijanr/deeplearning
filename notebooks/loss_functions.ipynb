{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss functions \n",
    "\n",
    "Loss functions are used to train neural networks. They measure the difference between the output of the network and the desired output. The loss function is a key part of the training process because it is the guide to the network about how to update the weights. The loss function takes in the (output, target) pair of inputs and computes a value that estimates how far away the output is from the target. The higher the loss value, the more different the output is from the target. The goal of training is to reduce this loss value.\n",
    "\n",
    "Different loss functions:\n",
    "\n",
    "   1. Mean Squared Error (MSE)\n",
    "   2. Mean Absolute Error (MAE)\n",
    "   3. Huber Loss\n",
    "   4. Cross Entropy Loss\n",
    "   5. Binary Cross Entropy Loss\n",
    "   6. Kullback-Leibler Divergence Loss\n",
    "   7. Hinge Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Mean Squared Error (MSE)\n",
    "\n",
    "MSE is the most commonly used regression loss function. MSE is the sum of squared distances between our target variable and predicted values. The MSE is a measure of the quality of an estimator. It is always non-negative, and values closer to zero are better.\n",
    "\n",
    "Equation:\n",
    "\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$$\n",
    "\n",
    "where $y_i$ is the target value and $\\hat{y_i}$ is the predicted value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6584520478375584\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "def mse_loss(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true - y_pred, 2))\n",
    "\n",
    "y_true = np.random.normal(0, 1, 100)\n",
    "y_pred = np.random.normal(0, 1, 100)\n",
    "print(mse_loss(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Mean Absolute Error (MAE)\n",
    "\n",
    "MAE is the average of the absolute difference between the target value and the value predicted by the model. It is the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n",
    "\n",
    "Equation:\n",
    "\n",
    "$$MAE = \\frac{1}{n}\\sum_{i=1}^{n}|y_i - \\hat{y_i}|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.024812570272987\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "def mae_loss(y_true, y_pred):\n",
    "    return np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "y_true = np.random.normal(0, 1, 100)\n",
    "y_pred = np.random.normal(0, 1, 100)\n",
    "print(mae_loss(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
